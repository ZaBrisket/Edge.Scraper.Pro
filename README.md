# EdgeScraperPro - Standalone HTML Version

A professional web scraper with sports data extraction capabilities, now running as a pure HTML + Netlify Functions application.

## üöÄ Quick Start

### Local Development

1. Install dependencies:
```bash
npm install
```

2. Run locally:
```bash
# Using http-server
npm run serve

# Using Netlify Dev (recommended)
npm run dev
```

3. Open http://localhost:8080 (or http://localhost:8888 with Netlify Dev)

### Deployment

This app is configured for Netlify deployment:

```bash
git push origin main
```

Netlify will automatically:
- Serve the `public/` directory as static files
- Deploy serverless functions from `netlify/functions/`
- Apply environment variables from netlify.toml

## üìÅ Project Structure

```
edge-scraper-pro/
‚îú‚îÄ‚îÄ public/                      # Static HTML application
‚îÇ   ‚îú‚îÄ‚îÄ index.html              # Main application
‚îÇ   ‚îú‚îÄ‚îÄ batch-processor.js      # Batch processing logic
‚îÇ   ‚îú‚îÄ‚îÄ enhanced-url-validator.js # URL validation
‚îÇ   ‚îú‚îÄ‚îÄ pfr-validator.js        # Sports-specific validation
‚îÇ   ‚îî‚îÄ‚îÄ sports-extractor.js     # Sports content extraction
‚îú‚îÄ‚îÄ netlify/
‚îÇ   ‚îî‚îÄ‚îÄ functions/              # Serverless functions
‚îÇ       ‚îú‚îÄ‚îÄ fetch-url.js        # Main scraping endpoint
‚îÇ       ‚îî‚îÄ‚îÄ health.js           # Health check
‚îú‚îÄ‚îÄ src/                        # Shared libraries (for functions)
‚îú‚îÄ‚îÄ netlify.toml               # Netlify configuration
‚îú‚îÄ‚îÄ package.json               # Minimal dependencies
‚îî‚îÄ‚îÄ .env                       # Local environment variables
```

## üîß Environment Variables

Set these in Netlify Dashboard > Site settings > Environment variables:

```bash
BYPASS_AUTH=true
HTTP_DEADLINE_MS=15000
PUBLIC_API_KEY=public-2024
NODE_ENV=production
```

## üéØ Features

- **URL Scraping**: Fetch and extract content from any public URL
- **Sports Mode**: Enhanced extraction for sports statistics sites
- **Batch Processing**: Process multiple URLs efficiently
- **Multiple Export Formats**: JSON, CSV, Excel
- **URL Validation**: Built-in validation for common patterns
- **Rate Limiting**: Respectful scraping with configurable limits

### M&A Target List Builder

Transform SourceScrub CSV exports into curated acquisition target lists:

1. Navigate to `/targets` or click "Target Lists" in the navigation
2. Upload a SourceScrub CSV export (handles 2-line header automatically) or v2 XLSX workbook
3. View curated "Target Universe" with:
   - Company logos from Clearbit
   - Derived fields (domain, revenue in $MM, executive block)
   - Interactive filters (State, Industry, End Market)
   - Column sorting and search
4. Export to CSV or Excel (includes IMAGE() formula for logos)

Features:
- **Privacy-first**: All processing happens client-side, emails excluded from exports by default
- **Data persistence**: Last session saved to localStorage
- **Smart parsing**: Handles SourceScrub format quirks (tab characters, 2-line header)
- **Excel compatibility**: Exports include Excel 365 IMAGE() formulas for logos

## üõ†Ô∏è API Endpoints

### Fetch URL
```
GET /.netlify/functions/fetch-url?url=https://example.com
Headers: X-API-Key: public-2024
```

### Health Check
```
GET /.netlify/functions/health
```

## üìù License

MIT